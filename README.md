# phishing-detection-albert  
Detecting phishing emails using ALBERT embeddings combined with deep neural network architectures.

# ğŸ§  Phishing Detection with ALBERT: Embedding-Powered Deep Learning

## ğŸ“Œ Project Overview  
This project applies ALBERT (`albert-base-v2`) for phishing email classification. ALBERT embeddings are extracted and fed into a series of deep learning classifiers â€” including RNN-LSTM and stacked LSTM networks â€” to detect phishing intent in emails. Extensive preprocessing, batch-wise embedding generation, and model evaluation with accuracy/loss plots and confusion matrices are included.

---

## ğŸ“‚ Dataset & Source

- **Input Datasets**: _(you may insert your own links)_
  - `set1`: Traditional labeled email dataset  
  - `set2`: Evaluation/testing email dataset  
  - `set3`: Public email phishing challenge data  
- **Format**:
  ```csv
  Text,label
  "Confirm your credentials now",1
  "Here's your payment receipt",0

ğŸ” Insights & Findings
âœ‚ï¸ Texts were cleaned using regex, stripped of digits and punctuations.

ğŸ”¤ ALBERT tokenizer was used with max length 128 and padded sequences.

ğŸ“Š Best-performing models leveraged stacked LSTM and RNN-LSTM over ALBERT embeddings.

ğŸ§  Embeddings were extracted using the [CLS] token and processed in batches via TensorFlow.

Model	Accuracy (Est.)
ANN	~94%
CNN	~95%
RNN-LSTM	~97%
Stacked LSTM	~98%

ğŸ¤– AI Pipeline Summary
Stage	Description
Preprocessing	Lowercasing, punctuation & digit removal (regex)
Embedding Extraction	ALBERT [CLS] token, batched with attention mask
Classifier Input	768-dim vectors per text
Model Evaluation	Accuracy, F1-score, confusion matrix, metric plots

ğŸ“ˆ Visualizations (Notebook Output)
âœ… Training vs Validation Loss plot

âœ… Training vs Validation Accuracy plot

âœ… Confusion Matrix for each model

âœ… All visualizations generated per model via matplotlib

ğŸ› ï¸ Models Implemented
ANN â€” Dense layers with dropout, sigmoid output

CNN â€” 1D convolution layers with flattening

RNN-LSTM â€” SimpleRNN â†’ LSTM with dropout

ANN-LSTM â€” Dense layers reshaped and fed to LSTM

CNN-LSTM â€” Conv1D followed by dual-layer LSTM

Stacked LSTM â€” Three stacked LSTM layers (128 â†’ 64 â†’ 32)

All models trained with:
â€¢ binary_crossentropy loss
â€¢ Adam optimizer
â€¢ Batch size: 64
â€¢ Early stopping (patience = 3)
